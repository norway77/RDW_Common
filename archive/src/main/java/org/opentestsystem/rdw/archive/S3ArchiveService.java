package org.opentestsystem.rdw.archive;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.Headers;
import com.amazonaws.services.s3.model.CopyObjectRequest;
import com.amazonaws.services.s3.model.DeleteObjectsRequest;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.S3ObjectSummary;
import com.amazonaws.util.IOUtils;
import com.google.common.base.CharMatcher;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.util.StreamUtils;

import java.io.BufferedInputStream;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.nio.file.Files;
import java.util.Map;
import java.util.Properties;
import java.util.stream.Collectors;

/**
 * An {@link ArchiveService} implementation that uses Amazon S3.
 *
 * @see ArchiveConfiguration
 */
public class S3ArchiveService implements ArchiveService {
    private static final Logger logger = LoggerFactory.getLogger(S3ArchiveService.class);
    private static final String S3SchemePrefix = "s3://";

    private final AmazonS3 amazonS3;
    private final String bucket;
    private final String objPrefix;
    private final String sse;

    /**
     * @param amazonS3 amazon S3 client
     * @param root root path, must start with "s3://", e.g. "s3://myBucket"
     * @param objPrefix objPrefix that is specific for a tenant
     * @param sse optional SSE algorithm, may be null, e.g. "AES256"
     */
    public S3ArchiveService(final AmazonS3 amazonS3, final String root, final String objPrefix, final String sse) {
        if (!validRoot(root)) {
            throw new IllegalArgumentException("Invalid S3 root " + root);
        }
        // just in case there is an errant trailing slash ...
        final String path = root.substring(S3SchemePrefix.length());
        bucket = path.endsWith("/") ? path.substring(0, path.length()-1) : path;

        if (!amazonS3.doesBucketExist(bucket)) {
            throw new IllegalArgumentException("Invalid S3 bucket " + bucket);
        }
        this.amazonS3 = amazonS3;
        this.sse = sse;
        this.objPrefix = objPrefix;
    }


    @Override
    public String getRawUri(final String location) {
        // "" empty prefix is the default
        return S3SchemePrefix + bucket + "/" + (objPrefix.isEmpty() ? "" : objPrefix + "/") + location;
    }

    @Override
    public void writeResource(final String location, final byte[] content, Properties properties) {
        if (properties == null) {
            properties = new Properties();
        }
        properties.put(Headers.CONTENT_LENGTH, (long) content.length);
        writeResource(location, new ByteArrayInputStream(content), properties);
    }

    @Override
    public void writeResource(final String location, final InputStream is, final Properties properties) {
        final ObjectMetadata metadata = mapPropertiesToObjectMetadata(properties);
        final boolean lengthKnown = (metadata.getContentLength() > 0);

        // it baffles me why this isn't part of the AmazonS3Client configuration
        // but it's not, so set the SSE algorithm in the metadata if specified ...
        if (sse != null) {
            metadata.setSSEAlgorithm(sse);
        }

        InputStream content = is;

        // if the content length is not specified, the AmazonS3Client will let the entire stream get loaded
        // into memory (BAD) so write all the content to a local file to get the length, then stream that ...
        File temp = null;
        if (!lengthKnown) {
            try {
                temp = Files.createTempFile(null, null).toFile();
                try (final OutputStream fos = new FileOutputStream(temp)) {
                    metadata.setContentLength(IOUtils.copy(is, fos));
                }
                content = new FileInputStream(temp);
            } catch (final IOException e) {
                logger.warn("Error writing temporary file", e);
                throw new RuntimeException(e);
            }
        }

        try {
            amazonS3.putObject(bucket, location, content, metadata);
        } catch (final AmazonServiceException e) {
            final String message = "Error writing content to " + getRawUri(location);
            logger.warn(message, e);
            throw new RuntimeException(message, e);
        } finally {
            // if a temporary file was created, close the stream and delete the file
            if (!lengthKnown) {
                try {
                    content.close();
                } catch (final IOException e) {
                    logger.info("Failed to close temporary file stream", e);
                }
                if (!temp.delete()) {
                    logger.info("Failed to clean up temporary file " + temp.getPath());
                }
            }
        }
    }

    @Override
    public byte[] readResource(final String location) {
        try (final InputStream is = openResource(location)) {
            return StreamUtils.copyToByteArray(new BufferedInputStream(is));
        } catch (final IOException e) {
            final String msg = "Error reading content from " + bucket + location;
            logger.warn(msg, e);
            throw new RuntimeException(msg, e);
        }
    }

    @Override
    public InputStream openResource(final String location) {
        try {
            return amazonS3.getObject(bucket, location).getObjectContent();
        } catch (final AmazonServiceException e) {
            if (e.getStatusCode() == 404 || e.getStatusCode() == 301) {
                throw new IllegalArgumentException("Invalid resource location " + location);
            }
            final String msg = "Amazon error reading content from " + bucket + location;
            logger.warn(msg, e);
            throw e;
        }
    }

    @Override
    public Properties readProperties(final String location) {
        final Properties properties = new Properties();

        final ObjectMetadata metadata = amazonS3.getObjectMetadata(bucket, location);
        for (final Map.Entry<String, String> entry : metadata.getUserMetadata().entrySet()) {
            properties.setProperty(entry.getKey(), entry.getValue());
        }
        // inject S3-specific properties
        if (metadata.getContentType() != null && !properties.containsKey(Headers.CONTENT_TYPE)) {
            properties.setProperty(Headers.CONTENT_TYPE, metadata.getContentType());
        }
        if (metadata.getContentLength() > 0 && !properties.containsKey(Headers.CONTENT_LENGTH)) {
            properties.put(Headers.CONTENT_LENGTH, metadata.getContentLength());
        }
        // inject the RawURI
        properties.put(ArchiveService.RawURI, getRawUri(location));

        return properties;
    }

    @Override
    public void writeProperties(final String location, final Properties properties) {
        final ObjectMetadata existingMetadata = amazonS3.getObjectMetadata(bucket, location);
        final ObjectMetadata newMetadata = mapPropertiesToObjectMetadata(properties, existingMetadata);

        final CopyObjectRequest request = new CopyObjectRequest(bucket, location, bucket, location)
                .withNewObjectMetadata(newMetadata);

        amazonS3.copyObject(request);
    }

    @Override
    public void delete(final String location) {
        ObjectListing listing = amazonS3.listObjects(bucket, location);
        if (listing.getObjectSummaries().isEmpty()) {
            throw new IllegalArgumentException("Invalid location " + location);
        }

        final DeleteObjectsRequest request = new DeleteObjectsRequest(bucket);
        while (true) {
            request.setKeys(listing.getObjectSummaries().stream()
                    .map(S3ObjectSummary::getKey)
                    .map(k -> new DeleteObjectsRequest.KeyVersion(k, null))
                    .collect(Collectors.toList()));

            amazonS3.deleteObjects(request);

            if (listing.isTruncated()) {
                listing = amazonS3.listNextBatchOfObjects(listing);
            } else {
                break;
            }
        }
    }

    @Override
    public boolean exists(final String location) {
        return amazonS3.doesObjectExist(bucket, location);
    }

    static boolean validRoot(final String root) {
        return root != null && root.toLowerCase().startsWith(S3SchemePrefix);
    }

    private ObjectMetadata mapPropertiesToObjectMetadata(final Properties properties) {
        return mapPropertiesToObjectMetadata(properties, new ObjectMetadata());
    }

    private ObjectMetadata mapPropertiesToObjectMetadata(final Properties properties, final ObjectMetadata metadata) {
        if (properties != null) {
            for (final Map.Entry<Object, Object> entry : properties.entrySet()) {
                final String key = (String) entry.getKey();

                // Extract S3-specific properties into appropriate metadata and all other
                // properties into user metadata. NOTE: S3 metadata must be ascii-only (because
                // the SDK uses REST) so discard any non-ascii values.
                if (Headers.CONTENT_TYPE.equalsIgnoreCase(key)) {
                    metadata.setContentType((String) entry.getValue());
                } else if (Headers.CONTENT_LENGTH.equalsIgnoreCase(key)) {
                    metadata.setContentLength((Long) entry.getValue());
                } else {
                    final String value = entry.getValue().toString();
                    if (CharMatcher.ascii().matchesAllOf(value)) {
                        metadata.addUserMetadata(key, value);
                    } else {
                        logger.info("Discarding non-ascii object metadata {}={}", key, value);
                    }
                }
            }
        }

        return metadata;
    }
}
